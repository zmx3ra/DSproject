\documentclass{article}
\usepackage{float}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{An Examination of Ischemic Stroke Data: Exploring Machine Learning Models to Predict Hospital Ratings and Risk Adjusted Rate. }

\begin{icmlauthorlist}
\icmlauthor{Rachel Kersey}{equal,yyy}
\icmlauthor{Helen Sparling}{equal,yyy,comp}
\end{icmlauthorlist}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in

\begin{abstract}
This paper examines the best machine learning models to use in our dataset of ischemic stroke. Specifically, we used four different machine learning models: decision tree classifier, random forest and k nearest neighbor to predict hospital ratings, and multiple linear regression/a separate random forest model to predict risk adjusted rate. The variety of models used gave a detailed picture of the models that performed best in predicting categorical and numeric variables. The results indicated that the best classification model for hospital ratings was the random forest model, as it produced an ensemble accuracy of 0.93. Further, the decision tree classifier also performed decently, giving an accuracy score of 0.92. For prediction of risk adjusted rate, the ensemble random forest also performed the best, producing a r-squared ensemble of 0.76. The multiple linear regression did not perform well whatsoever, with an r-squared of .005. Overall, this analysis gives a good idea of the best machine learning models to use when evaluating this dataset. 
\end{abstract}

]  % <-- end of the full-width block

\section{Introduction}


\section{Data}
\label{submission}

The data set used for this investigation is “Ischemic Stroke Mortality and Readmission Rates at 30-Days and Quality Ratings in CA Hospitals.” The dataset is from Data.gov, a reputable website that contains many different kinds of datasets across a range of topics. The hyperlink to the data set is given below. Simply put, this dataset contains information on risk-adjusted mortality and readmission rates at 30 days, as well as quality ratings and deaths/readmissions, for Ischemic Strokes treated in hospitals throughout the state of California. Importantly, the risk-adjusted 30 day mortality examines mortality rates for each datapoint as adjusted for the severity of the individuals condition (risk adjusted). This dataset is important as it shows how hospital ratings are related to stroke readmissions and deaths: and machine learning models predicting these variables could demonstrate the need for hospitals to increase their ratings in order to lower readmission or death rates. The URL is available here:

\begin{center}
\textbf{\href{https://catalog.data.gov/dataset/ischemic-stroke-30-day-mortality-and-30-day-readmission-rates-and-quality-ratings-for-ca-h-92036}{Stroke Dataset Link}}
\end{center}



The dataset contained a total of 9 variables, 5 of which were numeric, and 4 of which were categorical. Prior to analysis, the data was subsetted to include only the five most populous counties in the state of California. The breakdown of each variable is as follows:
\begin{itemize}
\item Year: quantitative discrete, year data was recorded\@. 
\item County: categorical, county data was recorded in.
\item Hospital: categorical, hospital data was recorded in.
\item OSHPDID: categorical, hospital ID, not important for research question).
\item Risk Adjusted Rate: quantitative continuous (float), risk rate adjusted for age, health severity, readmissions and deaths).
\item Number of Deaths/Readmissions: quantitative continuous (float), number of deaths/readmissions for hospital that year.
\item Number of Cases: quantitative continuous (float), number of hospital cases
\item Hospital Ratings: categorical, patient hospital rating.
\item Location: quantitative (float), hospital location coordinates. (Rachel Kersey)
%\item Keep your abstract brief and self-contained, one paragraph and roughly
%    4--6 sentences. Gross violations will require correction at the
%    camera-ready phase. The title should have content words capitalized.
\end{itemize}

%\subsection{Data Dimensions}

\textbf{Data Dimensions} The size and the shape of the data is 1,026 observations with 9 columns. (Helen Sparling)

\medskip

%Authors must provide their manuscripts in \textbf{PDF} format.
%Furthermore, please make sure that files contain only embedded Type-1 fonts
%(e.g.,~using the program \texttt{pdffonts} in linux or using
%File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
%might come from graphics files imported into the document.

%Authors using \textbf{Word} must convert their document to PDF\@. Most
%of the latest versions of Word have the facility to do this
%automatically. Submissions will not be accepted in Word format or any
%format other than PDF\@. Really. We're not joking. Don't send Word.

%Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
%Those using \texttt{latex} and \texttt{dvips} may need the following
%two commands:

%\footnotesize
%\begin{verbatim}
%dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
%ps2pdf paper.ps
%\end{verbatim}}
%It is a zero following the ``-G'', which tells dvips to use
%the config.pdf file. Newer \TeX\ distributions don't always need this
%option.

%Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
%results. This program avoids the Type-3 font problem, and supports more
%advanced features in the \texttt{microtype} package.

\textbf{Data Quality Notes:} The data had missing values that would cause problems moving into analysis and model construction. In order to address this issue, the missing values were replaced with np.Nan so no additional cleaning was needed. Missing data was aimed to not be deleted as that would have potentially skewed the results of later analysis and model construction. However, later on in the study, some models were giving errors due to the np.Nan, so we were forced to remove the rows that contained missing data. There were not many of these rows though, so we were still able to use models on a large portion of our data.

Data cleaning included: coercing columns with periods to numeric as a precaution using pd.to\_numeric(), removing unnecessary wording changing "30-day Readmission" to "Readmission" using the .str.replace(), splitting the year column to change the format from "2011-2012" to "2011", and changing the year column to numeric using the pd.to\_numeric(). Changing the year column required a few steps. The year column was split at the "-" character using the .st.split() function. Then, as only the start year was needed, not the end year, the first part of the resulting string was selected and kept in a new column, and the old column was deleted. This method was slightly more complex than likely necessary, but it was successful in the end and resulted in a "Year" column that was ready for analysis, visualization, and model construction. Together, all of the changes changes made data visualization simpler. Additionally, as there were no unnecessary dollar signs, commas, or other characters, no other substantial cleaning was necessary. 

As the original dataset was quite large and in order to simplify analysis to further focus the research question, a subset of the dataset including the 5 most populated counties was selected. The location column contained information about the coordinate points of each data entry. The dataset, however, already contained a different column concerning location in terms of County. County is a far more understandable variable to work with compared to coordinates points. Thus, we removed the "location" column as the information was unhelpful and redundant. (Helen Sparling)


There are some outliers in the dataset. Boxplots were made for the three numeric variables: risk adjusted rate, number of cases, and number of deaths/readmissions. All of these variables were right skewed. Future steps will involve transformations to attempt to fix the data.(Rachel Kersey)



\subsection{Context and Relevance}

The goal of this project is to investigate the relationships between year and county and how they relate to risk adjusted rate, deaths, readmission, and case counts in hospitals across the five most populous counties in California. Further, we aim to find beneficial machine learning models to utilize in predicting hospital ratings and risk adjusted rates. These models could be useful for future hospitals to use when they are predicting their own risk adjusted rate or hospital rating. This investigation is important from a public health standpoint, and provides helpful insight into hospital functioning in the California area. Understanding which hospitals have higher deaths and cases, as well as how hospital ratings affect cases, deaths, and re admissions, would help policy makers identify hospitals in need of improvement, and provide recommendations of how to improve patient care. (Helen and Rachel)



\section{Methods}
\subsection{Pre-Analysis}

The following sections outline our pre-analysis plan for our dataset.

\subsubsection{Refined Research Question}


First, barplots were used to review associations within our dataset. A correlation matrix was also used, but there were no strong correlations immediately apparent among the numeric variables within the dataset. 

After reviewing some associations in the data, indicators of a relationship between hospital ratings and risk adjusted rate, number of cases, and number of deaths/readmissions were found. Based on these findings, the research question was refined to: Predict hospital rating (“as expected”, “worse”, “better”) from hospital location, year, risk adjusted rate, deaths, readmission, and case count. This question was used to use Machine Learning models discussed in class. (Rachel)

\subsubsection{Data Splitting Plan}

For our data splitting, we plan to utilize an 80/20 train test split through a def(maxmin) function, creating target and feature variables, and using the "from sklearn.model selection import train test split" in our decision tree classifier and k nearest neighbors. For the random forest models, we used train size = int(.8*N) and then subset df [0:train size] in order to split the data into training and testing rows. We did not split the data for multiple linear regression. (Rachel)

\subsubsection{Feature Selection}
\label{author info}

Before running any machine learning, certain categorical variables were one-hot encoded. Variables that were one-hot encoded included County and measure (mortality or readmission). Measure was a particularly simple one to one-hot encode conceptually because of the binary nature of the variable. Similarly, readmission was a binary categorical variable, and thus one-hot encoding was simple and intuitive. We attempted to one-hot encode by hospital, but it quickly proved to be an ineffective way of dealing with the data due to the large number of hospitals in the dataset. As hospital was no longer a good predictor of location, county was a far better location-based variable to one-hot encode as there were far fewer counties than hospitals in the dataset. Numerical variables used will include risk adjusted rate, number of deaths/readmission, and number of cases. 

There were to main objectives for this project:

\textbf{Objective 1:} Create a model that predicts "Hospital Ratings" based on the predictor variables. 

\textbf{Objective 2:} Create a model that predicts the "Risk Adjusted Rate" based on the predictor variables. 

(Helen) 

\subsubsection{Model Selection Plan}

We will use a classification model for addressing objective 1 because the predicted variable is categorical. We will use a multiple linear regression model for objective 2 because the predicted variable is numeric. 

Specifically for objective 1, a decision tree classifier model will be used because the measure variable is a categorical variable. We will also use a K-nearest neighbor for objective 1 because these models can handle categorical targets, categorical predictors, so long as they are one-hot encoded (which was done previously). Lastly, we will use a random forest so we can predict hospital ratings on an ensemble of trees, which gives us a good prediction as lots of trees are used. We used bootstrapping in our random forest here.

Tree based models (random forest) will be used for objective 2. This will be helpful in predicting risk adjusted rates with an average of many trees. We used bootstrapping in this model. We also did a multiple linear regression model so we would have something to compare the random forest to. (Helen)

\subsubsection{Evaluation Strategy}

For our evaluation strategy, we plan to use accuracy to evaluate how well our model does for our random forest (hospital ratings), k nearest neighbors, and decision tree classifier. This entails using "from sklearn import tree", making a decision classifier and fitting the classifier, making predictions on the test set, then completing a confusion matrix with an accuracy formula. Further, for our multiple linear regression and random forest (risk adjusted rate), we plan use r squared to assess how well our model performed. (Rachel)

\subsection{Analysis}

\subsubsection{Model implementation}

\textbf{Decision Tree Classifier}: It is a classification model that measures the probability of a hospital being rated “as expected”, “worse”, or “better” based on independent variables we gave it. To set this up, we used a def(maxmin) function and set our x and y. Our x was our features, which we chose to be 'Risk Adjusted Rate', '\# of Deaths/Readmissions', "\# of Cases", "Year", "County", "Hospital", and "Measure". Our y, or target variable, was Hospital ratings. Next, we selected the numeric x variables for normalization, and then recombined them with the categorical x variables. We then split the data on an 80/20 train test split. Next, we one hot encoded all of the categorical variables (County, Measure, and Hospital). Then, we imported the decision tree classifier, created the classifier object, fit the classifier, and made predictions on the test set. We finally computed accuracy to evaluate how well our model did. (Rachel)

\textbf{Random forest}: This was a more complicated model we chose, and we used this to explore our secondary question, so we made a model where our target variable was ‘Risk Adjusted Rate’ and we used a multitude of features in this model. Here, our model built a lot of decision trees and the random forest averages our results. This is more accurate than just one decision tree. To build this model, we first split the data into training rows and testing rows. We then removed the rows with the target to finalize our df\_train and df\_test. Our next step bootstrapping, where we set T=1000, and we split the data again into an X\_train and y\_train, X\_test and y\_test, where the y’s used arcsinh, and the x code dropped the risk adjusted rate. We then re-one hot encoded our categorical columns because we were having issues with our code. We then used a for loop to generate a bootstrapping sample, computed rsq, and made/saved predictions. Our next step was building an ensemble predictor, and we printed our rsq. 
We did the same thing but predicted hospital rating, it just required a lot of online work and AI to help refine my code because it needed to be changed from arcsinh to a classifier random forest. 


We used random forests for classification on hospital ratings. This did include bootstrapping. To build it, we defined our target variable y and our features x (all variables except year and OSHPDID), one hot encoded the categorical columns, and did an 80/20 train test split on the data. Then, we imported RandomForestClassifier, fit the classifier, and printed the confusion matrix and accuracy.
(Rachel)

\textbf{Multiple Linear Regression}: This was used as a comparison model to the ensemble random forest, and it predicts risk adjusted rate utilizing the other numeric variables in our dataset as predictors. We first dropped NA’s from our numerical variables as it was causing issues in the code. Next, we used def mlr(x,y) to build our multiple linear regression function. We set our intercept equal to one, and set our X to be our intercept + predictors, and our y to be Risk Adjusted Rate. Finally, we printed our MLR coefficients and r-squared. (Rachel)


\textbf{K Nearest Neighbors}: This model was used to determine what class (Hospital rating: “as expected”, “worse”, or “better”) a new case will belong to based on the predictor variables. We first created the target and predictor variables. We used get\_dummies to address non-numeric variables. This produced boolean values, so we adjusted the dummy variables to be 0 and 1 rather than “True” and “False”, allowing us to then create the KNN model. Then, we split the data 80/20, and scaled the X\_test and X\_train so that values would be comparable. Next, we needed to determine the ideal k value using SEE across possible values of 1-50 for k. The ideal K was 1 to minimize classification error. This is a bit unusual, but it is possible that the ideal k can be 1 when the data is clean and lacks excessive noise. However, the model may be prone to overfitting. (Helen)



\subsection{Architecture/structure}

\textbf{Number of trees in random forest}: We set T=1000 for the ensemble random forest, which predicted risk adjusted rate.
\textbf{Optimal k for KNN}: k=1 (Helen and Rachel)

\subsection{Pre-processing Steps}

\textbf{Train/Validation/Test split}: We split the data into an 80/20 train test split, which allows us to train a good chunk of the data, while the other piece is used to evaluate performance. We used this method in our decision tree classifier, our random forest for hospital ratings, and our K nearest neighbors model. We followed the code from the random forest classifier class to split our data for random forest. (Rachel)

\textbf{Scaling}: We scaled the data for the KNN model so that their values would be comparable. Once this was completed, we were able to determine the K value that minimizes test classification error, and make the KNN model. We also used maxmin functions for our decision tree classifier. (Helen)

\textbf{One hot encoding}: We have one hot encoded measure (readmission/mortality), hospital rating, and county to use in our models. (Rachel)

\textbf{Feature Selection}: The features we used to build our model were 'Risk Adjusted Rate', '\# of Deaths/Readmissions', "\# of Cases", "Year", "County", "Hospital",  and "Measure" if we were predicting hospital ratings. We used these measures as we saw some interesting relationships in our preliminary graphs and wanted to thus incorporate these variables into the model. For our models where we predicted risk adjusted rate, in the random forest ensemble (for risk adjusted rate), we used all of our variables in our dataset because we were curious how keeping them all in would affect ensemble rsq. The features we used in our MLR model included all of our numeric variables, except for risk adjusted rate, which was our target. (Rachel)

\section{Evaluation Benchmark}

\subsection{Metrics Used}
\textbf{Accuracy}:
The accuracy for our decision tree classifier was 0.92.\\ (Rachel)
The accuracy for our KNN model was 0.93. (Helen)
The accuracy for our random forest (hospital ratings) model was 0.926. (Rachel)

\vspace{0.3cm}

\noindent\textbf{Confusion Matrix}

\begin{table}[h!]
\centering
\caption{KNN Confusion Matrix}
\label{tab:knn_confusion_matrix}
\begin{tabular}{p{3cm}|c|c|c|c}
\hline
\textbf{Actual / Predicted} & \textbf{Class 0} & \textbf{Class 1} & \textbf{Class 2} & \textbf{Class 3} \\
\hline
\textbf{Class 0} & 189 & 2 & 0 & 0 \\
\textbf{Class 1} & 6 & 1 & 0 & 0 \\
\textbf{Class 2} & 1 & 0 & 0 & 0 \\
\textbf{Class 3} & 5 & 0 & 0 & 2 \\
\hline
\end{tabular}
\end{table}
(Helen)

\begin{table}[h!]
\centering
\caption{MLR Confusion Matrix}
\label{tab:confusion_matrix}
\begin{tabular}{c|c|c|c}
\hline
\textbf{Actual / Predicted} & \textbf{Class 0} & \textbf{Class 1} & \textbf{Class 2} \\
\hline
\textbf{Class 0} & 182 & 4 & 1 \\
\textbf{Class 1} & 4 & 6 & 0 \\
\textbf{Class 2} & 7 & 0 & 1 \\
\hline
\end{tabular}
\end{table}
(Rachel)
\vspace{0.3cm}
\textbf{R-squared}: Our ensemble rsq for the ensemble random forest was 0.76, and our rsq for MLR was 0.0056.
(Helen and Rachel)
\usepackage{graphicx} % in preamble


\subsection{Model comparison}
The KNN model had the best accuracy. As aforementioned, however, the k value with the least test classification error was 1, which makes this model more prone to overfitting. For predicting risk adjusted rate, random forest has a far better r-squared value compared to the multiple linear regression model (0.76 and 0.0056, respectively). (Helen)

\subsection{Initial Results and Comparison}
\textbf{Interpretation of which model performed best}:For models predicting hospital ratings, k nearest neighbors performed the best in regards to accuracy, but the decision tree classifier accuracy was very close to k nearest neighbors. In addition, the decision tree classifier is less prone to overfitting vs. the k nearest neighbors because our optimal k is 1 for the KNN. Further, from our random forest model, we had an accuracy of 0.926, which was higher than the decision tree classifier, and therefore our best model.

For models predicting risk adjusted rate, MLR performed poorly with a low rsq. The random forest performed better with a significantly higher rsq. 

\section{Initial Results Analysis}

The random forest and decision tree classifier models performed better than the KNN and MLR models. Regarding the KNN model, this model was not as ideal, though it has a high accuracy (0.93) due to the fact that the ideal k for minimizing test classification error was k=1. This model, therefore, is prone to overfitting. The model presenting these characteristics because the cleaned data set was large, clean, and lacked excessive noise. MLR performed poorly because a linear model simply failed to capture the variance in the data. The trends were more complex. The random forest model performed better because this model is non-linear and random forests account for more complex patterns. (Helen)

The random forest was likely the best when it came to predicting hospital ratings, as it takes multiple trees and allows them to “vote” on the outcome, so there are many more trees contributing to the results.
(Rachel)

\section{Conclusion}
\subsection{Summary}

In this project, five total models were created to address the following questions: how can hospital ratings and risk adjusted rate be predicted based off predictor variables such as county, number of deaths/readmission, number of cases, etc. This question was split into two main objectives, the first predicting hospital ratings, and the second addressing risk adjusted rate. 

To address these two questions, we 

\subsection{Defense}
\subsection{Future \& Additional Work}
\subsection{Issues and Limitations}
There were initially some complications with the random forest. To be more specific, when predicting hospital ratings with random forests, we needed to change the arc sin h. However, we were able to remedy this as we found a different code syntax in the classwork for random forest classifiers of categorical varaibles, which still produced a fantastic accuracy and used a standard 80/20 test train split. Repeated errors were corrected by making slight adjustments to the one-hot encoding and the order of the code. This resolved the errors. Further, we planned to transform the risk adjusted rate variable for multiple linear regression because it was skewed to the right. After multiple attempts at transformation, including log, sqrt, and cubed root, the r-squared only reached .02. We concluded that this model is not a good fit for our data.
\section{Figures}

\begin{figure}[H]  % note the capital H
\centering
\includegraphics[width=0.4\textwidth]{MLR Graph.png}
\caption{MLR model results}
\label{fig:mlr_graph}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{KNN SEE.png}
\caption{KNN Classification Error vs. k}
\label{fig:mlr_graph}
\end{figure}





\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
