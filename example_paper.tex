%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{float}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Relationship between Ischemic Stroke 30-Day Mortality and 30-Day Readmission Rates, Hospital Quality Ratings, and Location.}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rachel Kersey}{equal,yyy}
\icmlauthor{Helen Sparling}{equal,yyy,comp}
\end{icmlauthorlist}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This paper examines the relationship between Ischemic Stroke 30-Day Mortality and 30-Day Readmission Rates, Hospital Quality Ratings, and Location. (More will be added after data description) (Rachel Kersey)
\end{abstract}

\section{Data Description}
\label{submission}

The dataset name is, “Ischemic Stroke 30-Day Mortality and 30-Day Readmission Rates and Quality Ratings for CA Hospitals.” We got it from Data.gov, which is a reputable source to pull data from. The hyperlink to the dataset is as follows. The purpose of this dataset is to show risk-adjusted 30 day mortality and readmission rates, as well as quality ratings and deaths/readmissions, for Ischemic Strokes treated in hospitals in California. This dataset is important as it shows how hospital ratings are related to stroke readmissions and deaths: and a possible correlation could demonstrate the need for hospitals to increase their ratings in order to lower readmission or death rates. The URL is available here:

\begin{center}
\textbf{\href{https://catalog.data.gov/dataset/ischemic-stroke-30-day-mortality-and-30-day-readmission-rates-and-quality-ratings-for-ca-h-92036}{Stroke Dataset Link}}
\end{center}



We investigated a rich dataset with 9 variables: 5 numeric, and 4 categorical. We created a subset to include only the five most populated counties in California. The variables are listed as follows:
\begin{itemize}
\item Year: quantitative discrete, year data was recorded)\@. 
\item County: categorical, county data was recorded in.
\item Hospital: categorical, hospital data was recorded in.
\item OSHPDID: categorical, hospital ID, not important for research question).
\item Risk Adjusted Rate: quantitative continuous (float), risk rate adjusted for age, health severity, readmissions and deaths).
\item Number of Deaths/Readmissions: quantitative continuous (float), number of deaths/readmissions for hospital that year.
\item Number of Cases: quantitative continuous (float), number of hospital cases
\item Hospital Ratings: categorical, patient hospital rating.
\item Location: quantitative (float), hospital location coordinates. (Rachel Kersey)
%\item Keep your abstract brief and self-contained, one paragraph and roughly
%    4--6 sentences. Gross violations will require correction at the
%    camera-ready phase. The title should have content words capitalized.
\end{itemize}

%\subsection{Data Dimensions}

\textbf{Data Dimensions} The size and the shape of the data is 1,026 observations with 9 columns. (Helen Sparling)

\medskip

%Authors must provide their manuscripts in \textbf{PDF} format.
%Furthermore, please make sure that files contain only embedded Type-1 fonts
%(e.g.,~using the program \texttt{pdffonts} in linux or using
%File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
%might come from graphics files imported into the document.

%Authors using \textbf{Word} must convert their document to PDF\@. Most
%of the latest versions of Word have the facility to do this
%automatically. Submissions will not be accepted in Word format or any
%format other than PDF\@. Really. We're not joking. Don't send Word.

%Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
%Those using \texttt{latex} and \texttt{dvips} may need the following
%two commands:

%\footnotesize
%\begin{verbatim}
%dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
%ps2pdf paper.ps
%\end{verbatim}}
%It is a zero following the ``-G'', which tells dvips to use
%the config.pdf file. Newer \TeX\ distributions don't always need this
%option.

%Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
%results. This program avoids the Type-3 font problem, and supports more
%advanced features in the \texttt{microtype} package.

\textbf{Data Quality Notes:} The data had missing values. They were replaced with np.Nan so no additional cleaning was needed. Data cleaning included: coercing columns with periods to numeric as a precaution, removing unnecessary wording, splitting the year column to change the format from "2011-2012" to "2011", and changing the year column to numeric. These changes will make data visualization simpler. There were no unnecessary dollar signs, commas, or other characters. The original data set was quite large, so the 5 most populated counties were subsetted to make the dataset more manageable for analysis. The location column was removed because it was not necessary to answer the research question. (Helen Sparling)


There are some outliers in the dataset. Boxplots were made for the three numeric variables: risk adjusted rate, number of cases, and number of deaths/readmissions. All of these variables were right skewed. Future steps will involve transformations to fix the data and curb the number of outliers. (Rachel Kersey)



\subsection{Context and Relevance}

Our project/goal is to investigate the relationships between year and county and how they relate to risk adjusted rate, deaths, readmissions, and case count. This investigation is important from a public health standpoint, and provides helpful insight into hospital functioning in the California area. Understanding which hospitals have higher deaths and cases, as well as how hospital ratings affect cases, deaths, and re admissions, would help policy makers identify hospitals in need of improvement, and provide recommendations of how to improve patient care. (Helen and Rachel)



\section{Pre-Analysis}

The following sections outline our pre-analysis plan for our dataset.

\subsection{Refined Research Question}


We first reviewed some associations in our data utilizing barplots, as well as a correlation matrix. We did not find high correlation between any of the numeric variables. 

After reviewing some associations in our data, we found indicators that there is a relationship between hospital ratings and risk adjusted rate, number of cases, and number of deaths/readmissions. Thus, this led us to refine our research question: Predict hospital rating (“as expected”, “worse”, “better”) from hospital location, year, risk adjusted rate, deaths, readmissions, and case count. This question allows us to explore with Machine Learning methods we have used in class. (Rachel)

\subsection{Data Splitting Plan}

For our data splitting, we plan to utilize an 80/20 train test split through a def(maxmin) function, creating target and feature variables, and using the "from sklearn.model selection import train test split." (Rachel)

\subsection{Feature Selection}
\label{author info}

Before running any machine learning, certain variables were one-hot encoded. Variables that were one-hot encoded included County and measure (mortality or readmission). We did not one-hot encode by hospital due to the large number of different hospitals. County was a more reasonable location-based variable to one-hot encode. Numerical variables used will include risk adjusted rate, number of deaths/readmission, and number of cases. 

Interactions between variables will be analyzed. We will predict hospital ratings based on multiple variables, both categorical and numeric. These predictor variables are hospital location (County), year, risk adjusted rate, death, re admissions, and case count. We will do the same for objective 2, using risk adjusted rate as the predicted variable instead of hospital ratings. (Helen) 

\subsection{Model Selection Plan}

We will use a classification model for addressing objective 1 because the predicted variable is categorical. We will use a linear regression model for objective 2 because the predicted variable is numeric. 

Specifically for objective 1, a logistic regression model will be used because the measure variable is a categorical variable. We will also use a K-nearest neighbor for objective 1 because these models can handle categorical targets, categorical predictors, so long as they are one-hot encoded (which was done previously). 

Tree based models will be used for objective 2. This will be helpful in making a model with mixed data types (numeric and categorical) without first hot encoding. (Helen)

\subsection{Evaluation Strategy}

For our evaluation strategy, we plan to use accuracy and precision to evaluate how well our model does. This entails using "from sklearn import tree", making a decision classifier and fitting the classifier, making predictions on the test set, then completing a confusion matrix with an accuracy formula. (Rachel)

\section{Analysis}

\subsection{Model implementation}

\textbf{Decision Tree Classifier}: It is a classification model that measures the probability of a hospital being rated “as expected”, “worse”, or “better” based on independent variables we gave it. To set this up, we used a def(maxmin) function and set our x and y. Our x was our features, which we chose to be 'Risk Adjusted Rate', '\# of Deaths/Readmissions', "\# of Cases", "Year", "County", "Hospital", and "Measure". Our y, or target variable, was Hospital ratings. Next, we selected the numeric x variables for normalization, and then recombined them with the categorical x variables. We then split the data on an 80/20 train test split. Next, we one hot encoded all of the categorical variables (County, Measure, and Hospital). Then, we imported the decision tree classifier, created the classifier object, fit the classifier, and made predictions on the test set. We finally computed accuracy to evaluate how well our model did. (Rachel)

\textbf{Random forest}: This was a more complicated model we chose, and we used this to explore our secondary question, so we made a model where our target variable was ‘Risk Adjusted Rate’ and we used a multitude of features in this model. Here, our model built a lot of decision trees and the random forest averages our results. This is more accurate than just one decision tree. To build this model, we first split the data into training rows and testing rows. We then removed the rows with the target to finalize our df\_train and df\_test. Our next step bootstrapping, where we set T=1000, and we split the data again into an X\_train and y\_train, X\_test and y\_test, where the y’s used arcsinh, and the x code dropped the risk adjusted rate. We then re-one hot encoded our categorical columns because we were having issues with our code. We then used a for loop to generate a bootstrapping sample, computed rsq, and made/saved predictions. Our next step was building an ensemble predictor, and we printed our rsq. 
We did the same thing but predicted hospital rating, it just required a lot of online work and AI to help refine my code because it needed to be changed from arcsinh to a classifier random forest. 


We also used random forests for classification on hospital ratings. This did not include bootstrapping. To build it, we defined our target variable y and our features x (all variables except year and OSHPDID), one hot encoded the categorical columns, and did an 80/20 train test split on the data. Then, we imported RandomForestClassifier, fit the classifier, and printed the confusion matrix and accuracy.
(Rachel)

\textbf{Multiple Linear Regression}: This was used as a comparison model to the ensemble random forest, and it predicts risk adjusted rate utilizing the other numeric variables in our dataset as predictors. We first dropped NA’s from our numerical variables as it was causing issues in the code. Next, we used def mlr(x,y) to build our multiple linear regression function. We set our intercept equal to one, and set our X to be our intercept + predictors, and our y to be Risk Adjusted Rate. Finally, we printed our MLR coefficients and r-squared. (Rachel)


\textbf{K Nearest Neighbors}: This model was used to determine what class (Hospital rating: “as expected”, “worse”, or “better”) a new case will belong to based on the predictor variables. We first created the target and predictor variables. We used get\_dummies to address non-numeric variables. This produced boolean values, so we adjusted the dummy variables to be 0 and 1 rather than “True” and “False”, allowing us to then create the KNN model. Then, we split the data 80/20, and scaled the X\_test and X\_train so that values would be comparable. Next, we needed to determine the ideal k value using SEE across possible values of 1-50 for k. The ideal K was 1 to minimize classification error. This is a bit unusual, but it is possible that the ideal k can be 1 when the data is clean and lacks excessive noise. However, the model may be prone to overfitting. (Helen)



\subsection{Architecture/structure}

\textbf{Number of trees in random forest}: We set T=1000 for the ensemble random forest, which predicted risk adjusted rate.
\textbf{Optimal k for KNN}: k=1 (Helen and Rachel)

\subsection{Pre-processing Steps}

\textbf{Train/Validation/Test split}: We split the data into an 80/20 train test split, which allows us to train a good chunk of the data, while the other piece is used to evaluate performance. We used this method in our decision tree classifier, our random forest for hospital ratings, and our K nearest neighbors model. We followed the code from the random forest classifier class to split our data for random forest. (Rachel)

\textbf{Scaling}: We scaled the data for the KNN model so that their values would be comparable. Once this was completed, we were able to determine the K value that minimizes test classification error, and make the KNN model. We also used maxmin functions for our decision tree classifier. (Helen)

\textbf{One hot encoding}: We have one hot encoded measure (readmission/mortality), hospital rating, and county to use in our models. (Rachel)

\textbf{Feature Selection}: The features we used to build our model were 'Risk Adjusted Rate', '\# of Deaths/Readmissions', "\# of Cases", "Year", "County", "Hospital",  and "Measure" if we were predicting hospital ratings. We used these measures as we saw some interesting relationships in our preliminary graphs and wanted to thus incorporate these variables into the model. For our models where we predicted risk adjusted rate, in the random forest ensemble (for risk adjusted rate), we used all of our variables in our dataset because we were curious how keeping them all in would affect ensemble rsq. The features we used in our MLR model included all of our numeric variables, except for risk adjusted rate, which was our target. (Rachel)

\section{Evaluation Benchmark}

\subsection{Metrics Used}
\textbf{Accuracy}:
The accuracy for our decision tree classifier was 0.92.\\ (Rachel)
The accuracy for our KNN model was 0.93. (Helen)
The accuracy for our random forest (hospital ratings) model was 0.926. (Rachel)

\vspace{0.3cm}

\noindent\textbf{Confusion Matrix}

\begin{table}[h!]
\centering
\caption{KNN Confusion Matrix}
\label{tab:knn_confusion_matrix}
\begin{tabular}{p{3cm}|c|c|c|c}
\hline
\textbf{Actual / Predicted} & \textbf{Class 0} & \textbf{Class 1} & \textbf{Class 2} & \textbf{Class 3} \\
\hline
\textbf{Class 0} & 189 & 2 & 0 & 0 \\
\textbf{Class 1} & 6 & 1 & 0 & 0 \\
\textbf{Class 2} & 1 & 0 & 0 & 0 \\
\textbf{Class 3} & 5 & 0 & 0 & 2 \\
\hline
\end{tabular}
\end{table}
(Helen)

\begin{table}[h!]
\centering
\caption{MLR Confusion Matrix}
\label{tab:confusion_matrix}
\begin{tabular}{c|c|c|c}
\hline
\textbf{Actual / Predicted} & \textbf{Class 0} & \textbf{Class 1} & \textbf{Class 2} \\
\hline
\textbf{Class 0} & 182 & 4 & 1 \\
\textbf{Class 1} & 4 & 6 & 0 \\
\textbf{Class 2} & 7 & 0 & 1 \\
\hline
\end{tabular}
\end{table}
(Rachel)
\vspace{0.3cm}
\textbf{R-squared}: Our ensemble rsq for the ensemble random forest was 0.76, and our rsq for MLR was 0.0056.
(Helen and Rachel)
\usepackage{graphicx} % in preamble


\subsection{Model comparison}
The KNN model had the best accuracy. As aforementioned, however, the k value with the least test classification error was 1, which makes this model more prone to overfitting. For predicting risk adjusted rate, random forest has a far better r-squared value compared to the multiple linear regression model (0.76 and 0.0056, respectively). (Helen)

\subsection{Initial Results and Comparison}
\textbf{Interpretation of which model performed best}:For models predicting hospital ratings, k nearest neighbors performed the best in regards to accuracy, but the decision tree classifier accuracy was very close to k nearest neighbors. In addition, the decision tree classifier is less prone to overfitting vs. the k nearest neighbors because our optimal k is 1 for the KNN. Further, from our random forest model, we had an accuracy of 0.926, which was higher than the decision tree classifier, and therefore our best model.

For models predicting risk adjusted rate, MLR performed poorly with a low rsq. The random forest performed better with a significantly higher rsq. 

\section{Initial Results Analysis}

The random forest and decision tree classifier models performed better than the KNN and MLR models. Regarding the KNN model, this one was not as ideal, though it has a high accuracy (0.93) due to the fact that the ideal k for minimizing test classification error was k=1. This model, therefore, is prone to overfitting. The model presenting these characteristics because the cleaned data set was large, clean, and lacked excessive noise. MLR performed poorly because a linear model simply failed to capture the variance in the data. The trends were more complex. The random forest model performed better because this model is non-linear and random forests account for more complex patterns. (Helen)

Random forest was likely the best when it came to predicting hospital ratings as it takes multiple trees and has them “vote” on the outcome, so there are many more trees contributing to the results.
(Rachel)

\section{Issues and Limitations}
We had some limitations when it came to ensemble learning for random forest as our code kept throwing errors. However, we just needed to make small adjustments to how we one-hot encoded and the order of our code, and it eventually worked. 
\section{Figures}

\begin{figure}[H]  % note the capital H
\centering
\includegraphics[width=0.4\textwidth]{MLR Graph.png}
\caption{MLR model results}
\label{fig:mlr_graph}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.4\textwidth]{KNN SEE.png}
\caption{KNN Classification Error vs. k}
\label{fig:mlr_graph}
\end{figure}





\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
